{
 "metadata": {
  "kernelspec": {
   "name": "glue_pyspark",
   "display_name": "Glue PySpark",
   "language": "python"
  },
  "language_info": {
   "name": "Python_Glue_Session",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "pygments_lexer": "python3",
   "file_extension": ".py"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showtags": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# AWS Glue Studio Notebook\n",
    "##### Glue job with PySpark to clean and transform citibike data and load into S3 data lake\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_timestamp, to_date, year, month, dayofmonth, hour, date_format, lit, when, count, avg, udf\n",
    ")\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "import math\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Citibike Data Processing\") \\\n",
    "    .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define Schema for the Dataset\n",
    "schema = StructType([\n",
    "    StructField(\"ride_id\", StringType(), True),\n",
    "    StructField(\"rideable_type\", StringType(), True),\n",
    "    StructField(\"started_at\", TimestampType(), True),\n",
    "    StructField(\"ended_at\", TimestampType(), True),\n",
    "    StructField(\"start_station_name\", StringType(), True),\n",
    "    StructField(\"start_station_id\", StringType(), True),\n",
    "    StructField(\"end_station_name\", StringType(), True),\n",
    "    StructField(\"end_station_id\", StringType(), True),\n",
    "    StructField(\"start_lat\", DoubleType(), True),\n",
    "    StructField(\"start_lng\", DoubleType(), True),\n",
    "    StructField(\"end_lat\", DoubleType(), True),\n",
    "    StructField(\"end_lng\", DoubleType(), True),\n",
    "    StructField(\"member_casual\", StringType(), True)\n",
    "])"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "raw",
   "source": [
    "# Load Raw Data from S3\n",
    "raw_data_path = \"s3://citibike-df/inbound-data/*.csv\"\n",
    "df = spark.read.schema(schema).option(\"header\", \"true\").csv(raw_data_path)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define Haversine Formula to Calculate Distance\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth radius in kilometers\n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = math.sin(dlat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "# Register Haversine function as a UDF\n",
    "haversine_udf = udf(haversine, DoubleType())"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Data Cleaning and Transformation\n",
    "processed_df = df.withColumnRenamed(\"member_casual\", \"user_type\") \\\n",
    "    .withColumn(\"trip_duration_seconds\", \n",
    "                (col(\"ended_at\").cast(\"long\") - col(\"started_at\").cast(\"long\"))) \\\n",
    "    .withColumn(\"start_year\", year(col(\"started_at\"))) \\\n",
    "    .withColumn(\"start_month\", month(col(\"started_at\"))) \\\n",
    "    .withColumn(\"start_day\", dayofmonth(col(\"started_at\"))) \\\n",
    "    .withColumn(\"start_hour\", hour(col(\"started_at\"))) \\\n",
    "    .withColumn(\"end_year\", year(col(\"ended_at\"))) \\\n",
    "    .withColumn(\"end_month\", month(col(\"ended_at\"))) \\\n",
    "    .withColumn(\"end_day\", dayofmonth(col(\"ended_at\"))) \\\n",
    "    .withColumn(\"end_hour\", hour(col(\"ended_at\"))) \\\n",
    "    .withColumn(\"user_type\", \n",
    "                when(col(\"user_type\") == \"member\", \"Subscriber\")\n",
    "                .otherwise(\"Customer\")) \\\n",
    "    .withColumn(\"start_station_id\", col(\"start_station_id\").cast(StringType())) \\\n",
    "    .withColumn(\"end_station_id\", col(\"end_station_id\").cast(StringType())) \\\n",
    "    .withColumn(\"start_lat\", col(\"start_lat\").cast(DoubleType())) \\\n",
    "    .withColumn(\"start_lng\", col(\"start_lng\").cast(DoubleType())) \\\n",
    "    .withColumn(\"end_lat\", col(\"end_lat\").cast(DoubleType())) \\\n",
    "    .withColumn(\"end_lng\", col(\"end_lng\").cast(DoubleType())) \\\n",
    "    .dropDuplicates([\"ride_id\"])  # Remove duplicate rows based on ride_id\n"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Handle Missing Values\n",
    "processed_df = processed_df.na.fill({\n",
    "    \"start_station_name\": \"Unknown\",\n",
    "    \"start_station_id\": \"Unknown\",\n",
    "    \"end_station_name\": \"Unknown\",\n",
    "    \"end_station_id\": \"Unknown\",\n",
    "    \"start_lat\": 0.0,\n",
    "    \"start_lng\": 0.0,\n",
    "    \"end_lat\": 0.0,\n",
    "    \"end_lng\": 0.0\n",
    "})"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Calculate Trip Distance using Haversine Formula\n",
    "processed_df = processed_df.withColumn(\n",
    "    \"trip_distance_km\",\n",
    "    haversine_udf(col(\"start_lat\"), col(\"start_lng\"), col(\"end_lat\"), col(\"end_lng\"))\n",
    ")"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Create Dimension Tables\n",
    "# 1. Stations Dimension\n",
    "stations = processed_df.select(\n",
    "    col(\"start_station_id\").alias(\"station_id\"),\n",
    "    col(\"start_station_name\").alias(\"station_name\"),\n",
    "    col(\"start_lat\").alias(\"latitude\"),\n",
    "    col(\"start_lng\").alias(\"longitude\")\n",
    ").distinct()"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 2. Users Dimension\n",
    "users = processed_df.select(\n",
    "    col(\"ride_id\").alias(\"user_id\"),  # Assuming ride_id can be used as user_id since we don't have the user data\n",
    "    lit(1990).alias(\"birth_year\"),    # Placeholder for birth year\n",
    "    lit(\"Unknown\").alias(\"gender\")    # Placeholder for gender\n",
    ").distinct()\n"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 3. Time Dimension\n",
    "# Set legacy time parser policy\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "time_dim = processed_df.select(\n",
    "    col(\"started_at\").alias(\"time_id\"),\n",
    "    year(col(\"started_at\")).alias(\"year\"),\n",
    "    month(col(\"started_at\")).alias(\"month\"),\n",
    "    dayofmonth(col(\"started_at\")).alias(\"day\"),\n",
    "    hour(col(\"started_at\")).alias(\"hour\"),\n",
    "    date_format(col(\"started_at\"), \"u\").cast(\"integer\").alias(\"day_of_week\"),  # 'u' for day of the week (1 = Monday, 7 = Sunday)\n",
    "    when(date_format(col(\"started_at\"), \"u\").isin([6, 7]), True).otherwise(False).alias(\"is_weekend\")\n",
    ").distinct()"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 4. Bikes Dimension (Placeholder)\n",
    "bikes = processed_df.select(\n",
    "    col(\"ride_id\").alias(\"bike_id\"),  # Assuming ride_id can be used as bike_id\n",
    "    lit(\"classic\").alias(\"bike_type\"),  # Placeholder for bike type\n",
    "    lit(\"Unknown\").alias(\"manufacturer\"),  # Placeholder for manufacturer\n",
    "    to_date(lit(\"2020-01-01\")).alias(\"purchase_date\")  # Cast string to DateType  # Placeholder for purchase date\n",
    ").distinct()"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 5. Weather Dimension (Placeholder)\n",
    "# Fetch weather data using an OpenWeatherMap API \n",
    "def fetch_weather(lat, lng, timestamp):\n",
    "    api_key = \"855152b9f44ec9bbf0c3c1f9d2f0b534\"\n",
    "    url = f\"https://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lng}&dt={timestamp}&appid={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        weather_data = response.json()\n",
    "        return {\n",
    "            \"temperature_c\": weather_data[\"main\"][\"temp\"] - 273.15,  # Convert Kelvin to Celsius\n",
    "            \"precipitation_mm\": weather_data.get(\"rain\", {}).get(\"1h\", 0.0),\n",
    "            \"weather_condition\": weather_data[\"weather\"][0][\"main\"]\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"temperature_c\": 0.0,\n",
    "            \"precipitation_mm\": 0.0,\n",
    "            \"weather_condition\": \"Unknown\"\n",
    "        }\n"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Create Weather Dimension\n",
    "# 1. Filter out rows with NULL time_id in weather_df\n",
    "weather_df = processed_df.select(\n",
    "    col(\"started_at\").alias(\"time_id\"),\n",
    "    col(\"start_lat\").alias(\"latitude\"),\n",
    "    col(\"start_lng\").alias(\"longitude\")\n",
    ").filter(col(\"started_at\").isNotNull()).distinct()  # Ensure time_id is not null\n"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "weather_df = weather_df.limit(5)"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# 2. Define schema with time_id as non-nullable\n",
    "weather_schema = StructType([\n",
    "    StructField(\"time_id\", TimestampType(), nullable=False),  # Primary key (non-null)\n",
    "    StructField(\"temperature_c\", DoubleType(), nullable=True),\n",
    "    StructField(\"precipitation_mm\", DoubleType(), nullable=True),\n",
    "    StructField(\"weather_condition\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "# 3. Fetch weather data and skip rows with invalid time_id\n",
    "weather_data = []\n",
    "for row in weather_df.collect():\n",
    "    # Skip rows where time_id is null (redundant due to filter above, but safe)\n",
    "    if row[\"time_id\"] is None:\n",
    "        continue\n",
    "    \n",
    "    weather = fetch_weather(row[\"latitude\"], row[\"longitude\"], int(row[\"time_id\"].timestamp()))\n",
    "    weather_data.append({\n",
    "        \"time_id\": row[\"time_id\"],\n",
    "        \"temperature_c\": weather[\"temperature_c\"],\n",
    "        \"precipitation_mm\": weather[\"precipitation_mm\"],\n",
    "        \"weather_condition\": weather[\"weather_condition\"]\n",
    "    })\n",
    "\n",
    "# 4. Create DataFrame with enforced schema and filter nulls\n",
    "weather_dim = spark.createDataFrame(weather_data, schema=weather_schema).filter(col(\"time_id\").isNotNull())\n"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# weather_dim.show()"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "text": "+-------------------+-------------+----------------+-----------------+\n|            time_id|temperature_c|precipitation_mm|weather_condition|\n+-------------------+-------------+----------------+-----------------+\n|2023-11-03 12:57:46|          0.0|             0.0|          Unknown|\n|2023-11-20 12:23:46|          0.0|             0.0|          Unknown|\n|2023-11-10 11:49:08|          0.0|             0.0|          Unknown|\n|2023-11-10 00:39:08|          0.0|             0.0|          Unknown|\n|2023-11-14 16:37:03|          0.0|             0.0|          Unknown|\n+-------------------+-------------+----------------+-----------------+\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Prepare Trip Fact Table\n",
    "trips = processed_df.select(\n",
    "    col(\"ride_id\").alias(\"trip_id\"),\n",
    "    col(\"start_station_id\"),\n",
    "    col(\"end_station_id\"),\n",
    "    col(\"started_at\").alias(\"start_time_id\"),\n",
    "    col(\"ended_at\").alias(\"end_time_id\"),\n",
    "    col(\"ride_id\").alias(\"user_id\"),  # Assuming ride_id can be used as user_id\n",
    "    col(\"ride_id\").alias(\"bike_id\"),  # Assuming ride_id can be used as bike_id\n",
    "    col(\"trip_duration_seconds\").cast(IntegerType()).alias(\"trip_duration_seconds\"),  # Cast to INT\n",
    "    col(\"trip_distance_km\"),\n",
    "    col(\"user_type\")\n",
    ").filter(col(\"ride_id\").isNotNull())  # Ensure start_time_id is not null"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# trips.show()"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Save Dimension Tables to S3\n",
    "dimensions_path = \"s3://citibike-df/outbound-data/dimensions/\"\n",
    "stations.write.mode(\"overwrite\").parquet(f\"{dimensions_path}stations_dim\")\n",
    "users.write.mode(\"overwrite\").parquet(f\"{dimensions_path}users_dim\")\n",
    "time_dim.write.mode(\"overwrite\").parquet(f\"{dimensions_path}time_dim\")\n",
    "bikes.write.mode(\"overwrite\").parquet(f\"{dimensions_path}bikes_dim\")\n",
    "weather_dim.write.mode(\"overwrite\").parquet(f\"{dimensions_path}weather_dim\")"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Save Fact Table to S3\n",
    "fact_path = \"s3://citibike-df/outbound-data/fact/\"\n",
    "trips.write.mode(\"overwrite\").parquet(f\"{fact_path}trips_fact\")"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Stop Spark Session\n",
    "spark.stop()"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# For debugging\n",
    "# spark = SparkSession.builder.appName(\"Parquet Inspector\").getOrCreate()\n",
    "\n",
    "# # Load the time_dim Parquet data\n",
    "# time_dim_df = spark.read.parquet(\"s3://citibike-df/outbound-data/fact/trips_fact/\")\n",
    "\n",
    "# # Print the schema\n",
    "# time_dim_df.printSchema()"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "text": "root\n |-- trip_id: string (nullable = true)\n |-- start_station_id: string (nullable = true)\n |-- end_station_id: string (nullable = true)\n |-- start_time_id: timestamp (nullable = true)\n |-- end_time_id: timestamp (nullable = true)\n |-- user_id: string (nullable = true)\n |-- bike_id: string (nullable = true)\n |-- trip_duration_seconds: integer (nullable = true)\n |-- trip_distance_km: double (nullable = true)\n |-- user_type: string (nullable = true)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
